{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# pylint: skip-file\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.datasets import (load_boston, load_breast_cancer, load_digits,\n",
    "                              load_iris, load_svmlight_file)\n",
    "from sklearn.metrics import log_loss, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "def multi_logloss(y_true, y_pred):\n",
    "    return np.mean([-math.log(y_pred[i][y]) for i, y in enumerate(y_true)])\n",
    "\n",
    "\n",
    "class TestEngine(unittest.TestCase):\n",
    "\n",
    "    def test_binary(self):\n",
    "        X, y = load_breast_cancer(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1,\n",
    "            'num_iteration': 50  # test num_iteration in dict here\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=20,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "        ret = log_loss(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 0.15)\n",
    "        self.assertEqual(len(evals_result['valid_0']['binary_logloss']), 50)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5)\n",
    "\n",
    "    def test_rf(self):\n",
    "        X, y = load_breast_cancer(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'boosting_type': 'rf',\n",
    "            'objective': 'binary',\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.5,\n",
    "            'feature_fraction': 0.5,\n",
    "            'num_leaves': 50,\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=50,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "        ret = log_loss(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 0.25)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['binary_logloss'][-1], ret, places=5)\n",
    "\n",
    "    def test_regression(self):\n",
    "        X, y = load_boston(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'metric': 'l2',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=50,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "        ret = mean_squared_error(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 16)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['l2'][-1], ret, places=5)\n",
    "\n",
    "    def test_missing_value_handle(self):\n",
    "        X_train = np.zeros((1000, 1))\n",
    "        y_train = np.zeros(1000)\n",
    "        trues = random.sample(range(1000), 200)\n",
    "        for idx in trues:\n",
    "            X_train[idx, 0] = np.nan\n",
    "            y_train[idx] = 1\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'metric': 'l2',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=20,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        ret = mean_squared_error(y_train, gbm.predict(X_train))\n",
    "        self.assertLess(ret, 0.005)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['l2'][-1], ret, places=5)\n",
    "\n",
    "    def test_missing_value_handle_na(self):\n",
    "        x = [0, 1, 2, 3, 4, 5, 6, 7, np.nan]\n",
    "        y = [1, 1, 1, 1, 0, 0, 0, 0, 1]\n",
    "\n",
    "        X_train = np.array(x).reshape(len(x), 1)\n",
    "        y_train = np.array(y)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'auc',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False,\n",
    "            'min_data': 1,\n",
    "            'num_leaves': 2,\n",
    "            'learning_rate': 1,\n",
    "            'min_data_in_bin': 1,\n",
    "            'zero_as_missing': False\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=1,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        pred = gbm.predict(X_train)\n",
    "        np.testing.assert_almost_equal(pred, y)\n",
    "\n",
    "    def test_missing_value_handle_zero(self):\n",
    "        x = [0, 1, 2, 3, 4, 5, 6, 7, np.nan]\n",
    "        y = [0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "        X_train = np.array(x).reshape(len(x), 1)\n",
    "        y_train = np.array(y)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'auc',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False,\n",
    "            'min_data': 1,\n",
    "            'num_leaves': 2,\n",
    "            'learning_rate': 1,\n",
    "            'min_data_in_bin': 1,\n",
    "            'zero_as_missing': True\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=1,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        pred = gbm.predict(X_train)\n",
    "        np.testing.assert_almost_equal(pred, y)\n",
    "\n",
    "    def test_missing_value_handle_none(self):\n",
    "        x = [0, 1, 2, 3, 4, 5, 6, 7, np.nan]\n",
    "        y = [0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "        X_train = np.array(x).reshape(len(x), 1)\n",
    "        y_train = np.array(y)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'auc',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False,\n",
    "            'min_data': 1,\n",
    "            'num_leaves': 2,\n",
    "            'learning_rate': 1,\n",
    "            'min_data_in_bin': 1,\n",
    "            'use_missing': False\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=1,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        pred = gbm.predict(X_train)\n",
    "        self.assertAlmostEqual(pred[0], pred[1], places=5)\n",
    "        self.assertAlmostEqual(pred[-1], pred[0], places=5)\n",
    "\n",
    "    def test_categorical_handle(self):\n",
    "        x = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        y = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "        X_train = np.array(x).reshape(len(x), 1)\n",
    "        y_train = np.array(y)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'auc',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False,\n",
    "            'min_data': 1,\n",
    "            'num_leaves': 2,\n",
    "            'learning_rate': 1,\n",
    "            'min_data_in_bin': 1,\n",
    "            'min_data_per_group': 1,\n",
    "            'cat_smooth': 1,\n",
    "            'cat_l2': 0,\n",
    "            'max_cat_to_onehot': 1,\n",
    "            'zero_as_missing': True,\n",
    "            'categorical_column': 0\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=1,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        pred = gbm.predict(X_train)\n",
    "        np.testing.assert_almost_equal(pred, y)\n",
    "\n",
    "    def test_categorical_handle2(self):\n",
    "        x = [0, np.nan, 0, np.nan, 0, np.nan]\n",
    "        y = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "        X_train = np.array(x).reshape(len(x), 1)\n",
    "        y_train = np.array(y)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'auc',\n",
    "            'verbose': -1,\n",
    "            'boost_from_average': False,\n",
    "            'min_data': 1,\n",
    "            'num_leaves': 2,\n",
    "            'learning_rate': 1,\n",
    "            'min_data_in_bin': 1,\n",
    "            'min_data_per_group': 1,\n",
    "            'cat_smooth': 1,\n",
    "            'cat_l2': 0,\n",
    "            'max_cat_to_onehot': 1,\n",
    "            'zero_as_missing': False,\n",
    "            'categorical_column': 0\n",
    "        }\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=1,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=True,\n",
    "                        evals_result=evals_result)\n",
    "        pred = gbm.predict(X_train)\n",
    "        np.testing.assert_almost_equal(pred, y)\n",
    "\n",
    "    def test_multiclass(self):\n",
    "        X, y = load_digits(10, True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_logloss',\n",
    "            'num_class': 10,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, params=params)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, params=params)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=50,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "        ret = multi_logloss(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 0.2)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['multi_logloss'][-1], ret, places=5)\n",
    "\n",
    "    def test_multiclass_prediction_early_stopping(self):\n",
    "        X, y = load_digits(10, True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_logloss',\n",
    "            'num_class': 10,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, params=params)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, params=params)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=50,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "\n",
    "        pred_parameter = {\"pred_early_stop\": True, \"pred_early_stop_freq\": 5, \"pred_early_stop_margin\": 1.5}\n",
    "        ret = multi_logloss(y_test, gbm.predict(X_test, **pred_parameter))\n",
    "        self.assertLess(ret, 0.8)\n",
    "        self.assertGreater(ret, 0.5)  # loss will be higher than when evaluating the full model\n",
    "\n",
    "        pred_parameter = {\"pred_early_stop\": True, \"pred_early_stop_freq\": 5, \"pred_early_stop_margin\": 5.5}\n",
    "        ret = multi_logloss(y_test, gbm.predict(X_test, **pred_parameter))\n",
    "        self.assertLess(ret, 0.2)\n",
    "\n",
    "    def test_early_stopping(self):\n",
    "        X, y = load_breast_cancer(True)\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        valid_set_name = 'valid_set'\n",
    "        # no early stopping\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=10,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        valid_names=valid_set_name,\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=5)\n",
    "        self.assertEqual(gbm.best_iteration, 10)\n",
    "        self.assertIn(valid_set_name, gbm.best_score)\n",
    "        self.assertIn('binary_logloss', gbm.best_score[valid_set_name])\n",
    "        # early stopping occurs\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        valid_names=valid_set_name,\n",
    "                        verbose_eval=False,\n",
    "                        early_stopping_rounds=5)\n",
    "        self.assertLessEqual(gbm.best_iteration, 100)\n",
    "        self.assertIn(valid_set_name, gbm.best_score)\n",
    "        self.assertIn('binary_logloss', gbm.best_score[valid_set_name])\n",
    "\n",
    "    def test_continue_train(self):\n",
    "        X, y = load_boston(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'l1',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)\n",
    "        init_gbm = lgb.train(params, lgb_train, num_boost_round=20)\n",
    "        model_name = 'model.txt'\n",
    "        init_gbm.save_model(model_name)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=30,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        # test custom eval metrics\n",
    "                        feval=(lambda p, d: ('mae', mean_absolute_error(p, d.get_label()), False)),\n",
    "                        evals_result=evals_result,\n",
    "                        init_model='model.txt')\n",
    "        ret = mean_absolute_error(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 3.5)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['l1'][-1], ret, places=5)\n",
    "        for l1, mae in zip(evals_result['valid_0']['l1'], evals_result['valid_0']['mae']):\n",
    "            self.assertAlmostEqual(l1, mae, places=5)\n",
    "        os.remove(model_name)\n",
    "\n",
    "    def test_continue_train_multiclass(self):\n",
    "        X, y = load_iris(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_logloss',\n",
    "            'num_class': 3,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, params=params, free_raw_data=False)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, params=params, free_raw_data=False)\n",
    "        init_gbm = lgb.train(params, lgb_train, num_boost_round=20)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=30,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result,\n",
    "                        init_model=init_gbm)\n",
    "        ret = multi_logloss(y_test, gbm.predict(X_test))\n",
    "        self.assertLess(ret, 1.5)\n",
    "        self.assertAlmostEqual(evals_result['valid_0']['multi_logloss'][-1], ret, places=5)\n",
    "\n",
    "    def test_cv(self):\n",
    "        X, y = load_boston(True)\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {'verbose': -1}\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        # shuffle = False, override metric in params\n",
    "        params_with_metric = {'metric': 'l2', 'verbose': -1}\n",
    "        lgb.cv(params_with_metric, lgb_train, num_boost_round=10, nfold=3, stratified=False, shuffle=False,\n",
    "               metrics='l1', verbose_eval=False)\n",
    "        # shuffle = True, callbacks\n",
    "        lgb.cv(params, lgb_train, num_boost_round=10, nfold=3, stratified=False, shuffle=True,\n",
    "               metrics='l1', verbose_eval=False,\n",
    "               callbacks=[lgb.reset_parameter(learning_rate=lambda i: 0.1 - 0.001 * i)])\n",
    "        # self defined folds\n",
    "        tss = TimeSeriesSplit(3)\n",
    "        folds = tss.split(X_train)\n",
    "        lgb.cv(params_with_metric, lgb_train, num_boost_round=10, folds=folds, stratified=False, verbose_eval=False)\n",
    "        # lambdarank\n",
    "        X_train, y_train = load_svmlight_file(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../examples/lambdarank/rank.train'))\n",
    "        q_train = np.loadtxt(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../examples/lambdarank/rank.train.query'))\n",
    "        params_lambdarank = {'objective': 'lambdarank', 'verbose': -1}\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, group=q_train)\n",
    "        lgb.cv(params_lambdarank, lgb_train, num_boost_round=10, nfold=3, stratified=False, metrics='l2', verbose_eval=False)\n",
    "\n",
    "    def test_feature_name(self):\n",
    "        X, y = load_boston(True)\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {'verbose': -1}\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        feature_names = ['f_' + str(i) for i in range(13)]\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round=5, feature_name=feature_names)\n",
    "        self.assertListEqual(feature_names, gbm.feature_name())\n",
    "        # test feature_names with whitespaces\n",
    "        feature_names_with_space = ['f ' + str(i) for i in range(13)]\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round=5, feature_name=feature_names_with_space)\n",
    "        self.assertListEqual(feature_names, gbm.feature_name())\n",
    "\n",
    "    def test_save_load_copy_pickle(self):\n",
    "        def test_template(init_model=None, return_model=False):\n",
    "            X, y = load_boston(True)\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'l2',\n",
    "                'verbose': -1\n",
    "            }\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "            lgb_train = lgb.Dataset(X_train, y_train)\n",
    "            gbm_template = lgb.train(params, lgb_train, num_boost_round=10, init_model=init_model)\n",
    "            return gbm_template if return_model else mean_squared_error(y_test, gbm_template.predict(X_test))\n",
    "        gbm = test_template(return_model=True)\n",
    "        ret_origin = test_template(init_model=gbm)\n",
    "        other_ret = []\n",
    "        gbm.save_model('lgb.model')\n",
    "        other_ret.append(test_template(init_model='lgb.model'))\n",
    "        gbm_load = lgb.Booster(model_file='lgb.model')\n",
    "        other_ret.append(test_template(init_model=gbm_load))\n",
    "        other_ret.append(test_template(init_model=copy.copy(gbm)))\n",
    "        other_ret.append(test_template(init_model=copy.deepcopy(gbm)))\n",
    "        with open('lgb.pkl', 'wb') as f:\n",
    "            pickle.dump(gbm, f)\n",
    "        with open('lgb.pkl', 'rb') as f:\n",
    "            gbm_pickle = pickle.load(f)\n",
    "        other_ret.append(test_template(init_model=gbm_pickle))\n",
    "        gbm_pickles = pickle.loads(pickle.dumps(gbm))\n",
    "        other_ret.append(test_template(init_model=gbm_pickles))\n",
    "        for ret in other_ret:\n",
    "            self.assertAlmostEqual(ret_origin, ret, places=5)\n",
    "\n",
    "    @unittest.skipIf(not lgb.compat.PANDAS_INSTALLED, 'pandas is not installed')\n",
    "    def test_pandas_categorical(self):\n",
    "        import pandas as pd\n",
    "        X = pd.DataFrame({\"A\": np.random.permutation(['a', 'b', 'c', 'd'] * 75),  # str\n",
    "                          \"B\": np.random.permutation([1, 2, 3] * 100),  # int\n",
    "                          \"C\": np.random.permutation([0.1, 0.2, -0.1, -0.1, 0.2] * 60),  # float\n",
    "                          \"D\": np.random.permutation([True, False] * 150)})  # bool\n",
    "        y = np.random.permutation([0, 1] * 150)\n",
    "        X_test = pd.DataFrame({\"A\": np.random.permutation(['a', 'b', 'e'] * 20),\n",
    "                               \"B\": np.random.permutation([1, 3] * 30),\n",
    "                               \"C\": np.random.permutation([0.1, -0.1, 0.2, 0.2] * 15),\n",
    "                               \"D\": np.random.permutation([True, False] * 30)})\n",
    "        for col in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            X[col] = X[col].astype('category')\n",
    "            X_test[col] = X_test[col].astype('category')\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X, y)\n",
    "        gbm0 = lgb.train(params, lgb_train, num_boost_round=10, verbose_eval=False)\n",
    "        pred0 = list(gbm0.predict(X_test))\n",
    "        lgb_train = lgb.Dataset(X, y)\n",
    "        gbm1 = lgb.train(params, lgb_train, num_boost_round=10, verbose_eval=False,\n",
    "                         categorical_feature=[0])\n",
    "        pred1 = list(gbm1.predict(X_test))\n",
    "        lgb_train = lgb.Dataset(X, y)\n",
    "        gbm2 = lgb.train(params, lgb_train, num_boost_round=10, verbose_eval=False,\n",
    "                         categorical_feature=['A'])\n",
    "        pred2 = list(gbm2.predict(X_test))\n",
    "        lgb_train = lgb.Dataset(X, y)\n",
    "        gbm3 = lgb.train(params, lgb_train, num_boost_round=10, verbose_eval=False,\n",
    "                         categorical_feature=['A', 'B', 'C', 'D'])\n",
    "        pred3 = list(gbm3.predict(X_test))\n",
    "        gbm3.save_model('categorical.model')\n",
    "        gbm4 = lgb.Booster(model_file='categorical.model')\n",
    "        pred4 = list(gbm4.predict(X_test))\n",
    "        np.testing.assert_almost_equal(pred0, pred1)\n",
    "        np.testing.assert_almost_equal(pred0, pred2)\n",
    "        np.testing.assert_almost_equal(pred0, pred3)\n",
    "        np.testing.assert_almost_equal(pred0, pred4)\n",
    "\n",
    "    def test_reference_chain(self):\n",
    "        X = np.random.normal(size=(100, 2))\n",
    "        y = np.random.normal(size=100)\n",
    "        tmp_dat = lgb.Dataset(X, y)\n",
    "        # take subsets and train\n",
    "        tmp_dat_train = tmp_dat.subset(np.arange(80))\n",
    "        tmp_dat_val = tmp_dat.subset(np.arange(80, 100)).subset(np.arange(18))\n",
    "        params = {'objective': 'regression_l2', 'metric': 'rmse'}\n",
    "        gbm = lgb.train(params, tmp_dat_train, num_boost_round=20, valid_sets=[tmp_dat_train, tmp_dat_val])\n",
    "\n",
    "    def test_contribs(self):\n",
    "        X, y = load_breast_cancer(True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        evals_result = {}\n",
    "        gbm = lgb.train(params, lgb_train,\n",
    "                        num_boost_round=20,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        verbose_eval=False,\n",
    "                        evals_result=evals_result)\n",
    "\n",
    "        self.assertLess(np.linalg.norm(gbm.predict(X_test, raw_score=True) - np.sum(gbm.predict(X_test, pred_contrib=True), axis=1)), 1e-4)\n",
    "\n",
    "    def test_sliced_data(self):\n",
    "        def train_and_get_predictions(features, labels):\n",
    "            dataset = lgb.Dataset(features, label=labels)\n",
    "            lgb_params = {\n",
    "                'application': 'binary',\n",
    "                'verbose': -1,\n",
    "                'min_data': 5,\n",
    "            }\n",
    "            lgbm_model = lgb.train(\n",
    "                params=lgb_params,\n",
    "                train_set=dataset,\n",
    "                num_boost_round=10,\n",
    "            )\n",
    "            predictions = lgbm_model.predict(features)\n",
    "            return predictions\n",
    "        num_samples = 100\n",
    "        features = np.random.rand(num_samples, 5)\n",
    "        positive_samples = int(num_samples * 0.25)\n",
    "        labels = np.append(\n",
    "            np.ones(positive_samples, dtype=np.float32),\n",
    "            np.zeros(num_samples - positive_samples, dtype=np.float32),\n",
    "        )\n",
    "        # test sliced labels\n",
    "        origin_pred = train_and_get_predictions(features, labels)\n",
    "        stacked_labels = np.column_stack((labels, np.ones(num_samples, dtype=np.float32)))\n",
    "        sliced_labels = stacked_labels[:, 0]\n",
    "        sliced_pred = train_and_get_predictions(features, sliced_labels)\n",
    "        np.testing.assert_almost_equal(origin_pred, sliced_pred)\n",
    "        # append some columns\n",
    "        stacked_features = np.column_stack((np.ones(num_samples, dtype=np.float32), features))\n",
    "        stacked_features = np.column_stack((np.ones(num_samples, dtype=np.float32), stacked_features))\n",
    "        stacked_features = np.column_stack((stacked_features, np.ones(num_samples, dtype=np.float32)))\n",
    "        stacked_features = np.column_stack((stacked_features, np.ones(num_samples, dtype=np.float32)))\n",
    "        # append some rows\n",
    "        stacked_features = np.concatenate((np.ones(9, dtype=np.float32).reshape((1, 9)), stacked_features), axis=0)\n",
    "        stacked_features = np.concatenate((np.ones(9, dtype=np.float32).reshape((1, 9)), stacked_features), axis=0)\n",
    "        stacked_features = np.concatenate((stacked_features, np.ones(9, dtype=np.float32).reshape((1, 9))), axis=0)\n",
    "        stacked_features = np.concatenate((stacked_features, np.ones(9, dtype=np.float32).reshape((1, 9))), axis=0)\n",
    "        # test sliced 2d matrix\n",
    "        sliced_features = stacked_features[2:102, 2: 7]\n",
    "        assert np.all(sliced_features == features)\n",
    "        sliced_pred = train_and_get_predictions(sliced_features, sliced_labels)\n",
    "        np.testing.assert_almost_equal(origin_pred, sliced_pred)\n",
    "        # test sliced CSR\n",
    "        stacked_csr = csr_matrix(stacked_features)\n",
    "        sliced_csr = stacked_csr[2:102, 2: 7]\n",
    "        assert np.all(sliced_csr == features)\n",
    "        sliced_pred = train_and_get_predictions(sliced_csr, sliced_labels)\n",
    "        np.testing.assert_almost_equal(origin_pred, sliced_pred)\n",
    "\n",
    "    def test_monotone_constraint(self):\n",
    "        def is_increasing(y):\n",
    "            return np.count_nonzero(np.diff(y) < 0.0) == 0\n",
    "\n",
    "        def is_decreasing(y):\n",
    "            return np.count_nonzero(np.diff(y) > 0.0) == 0\n",
    "\n",
    "        def is_correctly_constrained(learner):\n",
    "            n = 200\n",
    "            variable_x = np.linspace(0, 1, n).reshape((n, 1))\n",
    "            fixed_xs_values = np.linspace(0, 1, n)\n",
    "            for i in range(n):\n",
    "                fixed_x = fixed_xs_values[i] * np.ones((n, 1))\n",
    "                monotonically_increasing_x = np.column_stack((variable_x, fixed_x))\n",
    "                monotonically_increasing_y = learner.predict(monotonically_increasing_x)\n",
    "                monotonically_decreasing_x = np.column_stack((fixed_x, variable_x))\n",
    "                monotonically_decreasing_y = learner.predict(monotonically_decreasing_x)\n",
    "                if not (is_increasing(monotonically_increasing_y) and is_decreasing(monotonically_decreasing_y)):\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        number_of_dpoints = 3000\n",
    "        x1_positively_correlated_with_y = np.random.random(size=number_of_dpoints)\n",
    "        x2_negatively_correlated_with_y = np.random.random(size=number_of_dpoints)\n",
    "        x = np.column_stack((x1_positively_correlated_with_y, x2_negatively_correlated_with_y))\n",
    "        zs = np.random.normal(loc=0.0, scale=0.01, size=number_of_dpoints)\n",
    "        y = (5 * x1_positively_correlated_with_y\n",
    "             + np.sin(10 * np.pi * x1_positively_correlated_with_y)\n",
    "             - 5 * x2_negatively_correlated_with_y\n",
    "             - np.cos(10 * np.pi * x2_negatively_correlated_with_y)\n",
    "             + zs)\n",
    "        trainset = lgb.Dataset(x, label=y)\n",
    "        params = {\n",
    "            'min_data': 20,\n",
    "            'num_leaves': 20,\n",
    "            'monotone_constraints': '1,-1'\n",
    "        }\n",
    "        constrained_model = lgb.train(params, trainset)\n",
    "        assert is_correctly_constrained(constrained_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}